lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
table(correct = correct_classes, predicted = lda.pred$class) %>% prop.table()
table(correct = correct_classes, predicted = lda.pred$class)
table(correct = correct_classes, predicted = lda.pred$class) %>% prop.table()
table(correct = correct_classes, predicted = lda.pred$class)
table(correct = correct_classes, predicted = lda.pred$class) %>% prop.table() %>% addmargins()
scale(Boston)
boston_scaled2 <- scale(Boston)
boston_scaled2 <- scale(Boston)
summary(boston_scaled2)
dist_eu <- dist(boston_scaled2)
dist_eu <- dist(boston_scaled2)
dist_eu
dist_eu <- dist(boston_scaled2)
km <- kmeans(boston_scaled2, centers = 3)
km <- kmeans(boston_scaled2, centers = 3)
km
km <- kmeans(boston_scaled2, centers = 3)
# avoid the kmeans to give us every time different results
set.seed(123)
# set the maximum number of clusterss to 10
k_max <- 10
# calculate the total sum of squares:
twcss <- sapply(1:k_max, function(k){kmeans (boston_scaled2, k)$tot.wthinss})
# see graphically the optimal number of clusters:
qplot(x = 1:kmax, y= twcss, geom = "line")
# avoid the kmeans to give us every time different results
set.seed(123)
# set the maximum number of clusterss to 10
k_max <- 10
# calculate the total sum of squares:
twcss <- sapply(1:k_max, function(k){kmeans (boston_scaled2, k)$tot.wthinss})
# see graphically the optimal number of clusters:
qplot(x = 1:k_max, y= twcss, geom = "line")
# avoid the kmeans to give us every time different results
set.seed(123)
# set the maximum number of clusters to 10
k_max <- 10
# calculate the total sum of squares:
twcss <- sapply(1:k_max, function(k){kmeans (boston_scaled2, k)$tot.wthinss})
# see graphically the optimal number of clusters:
qplot(x = 1:k_max, y= twcss, geom = "line")
# avoid the kmeans to give us every time different results
set.seed(123)
# set the maximum number of clusters to 10
k_max <- 10
# calculate the total sum of squares:
twcss <- sapply(1:k_max, function(k){kmeans (boston_scaled2, k)$tot.wthinss})
# see graphically the optimal number of clusters:
qplot(x = 1:k_max, y= twcss, geom = 'line')
# avoid the kmeans to give us every time different results
set.seed(123)
# set the maximum number of clusters to 10
k_max <- 10
# calculate the total sum of squares:
twcss <- sapply(1:k_max, function(k){kmeans (boston_scaled2, k)$tot.wthinss})
str(Boston)
install.packages("MASS")
library(MASS)
str(Boston)
dim(Boston)
install.packages("corrplot")
install.packages("tidyverse")
library(corrplot)
library(tidyverse)
cor_matrix <- cor(Boston) %>% round(digits=2)
corrplot.mixed(cor_matrix, lower.col = "black", number.cex = .6)
summary(Boston)
gather(Boston) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()
boston_scaled <- scale(Boston)
summary(boston_scaled)
boston_scaled <- as.data.frame(boston_scaled)
# First we create the quantile vector of crim:
bins <- quantile(boston_scaled$crim)
bins
# Then we create the categorical variable "crime":
crime <- cut(boston_scaled$crim, breaks = bins, inlcude.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))
table(crime)
n <- nrow(boston_scaled)
# Choose randomly 80% of the rows:
ind <- sample(n, size = n*0.8)
# create the train set;
train <- boston_scaled[ind,]
# create the test set:
test <- boston_scaled[-ind,]
lda.fit <- lda(crime ~ ., data = train)
lda.fit <- lda(crime ~ ., data = train)
n <- nrow(boston_scaled)
# Choose randomly 80% of the rows:
ind <- sample(n, size = n*0.8)
# create the train set;
train <- boston_scaled[ind,]
# create the test set:
test <- boston_scaled[-ind,]
lda.fit <- lda(crime ~ ., data = train)
lda.fit <- lda(crime ~., data = train)
```{r LDA}
n <- nrow(boston_scaled)
# Choose randomly 80% of the rows:
ind <- sample(n, size = n*0.8)
# create the train set;
train <- boston_scaled[ind,]
# create the test set:
test <- boston_scaled[-ind,]
lda.fit <- lda(crime ~., data = train)
lda.fit <- lda(crime ~., data = train)
lda.fit <- lda(crime ~., data = train)
lda.fit <- lda(crime ~., data = train)
lda.fit <- lda(crime ~., data = train)
lda.fit <- lda(crime ~., data = train)
install.packages("MASS")
library(MASS)
str(Boston)
dim(Boston)
install.packages("corrplot")
install.packages("tidyverse")
library(corrplot)
library(tidyverse)
cor_matrix <- cor(Boston) %>% round(digits=2)
corrplot.mixed(cor_matrix, lower.col = "black", number.cex = .6)
summary(Boston)
gather(Boston) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()
boston_scaled <- scale(Boston)
summary(boston_scaled)
boston_scaled <- as.data.frame(boston_scaled)
# First we create the quantile vector of crim:
bins <- quantile(boston_scaled$crim)
bins
# Then we create the categorical variable "crime":
crime <- cut(boston_scaled$crim, breaks = bins, inlcude.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))
table(crime)
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
n <- nrow(boston_scaled)
# Choose randomly 80% of the rows:
ind <- sample(n, size = n*0.8)
# create the train set;
train <- boston_scaled[ind,]
# create the test set:
test <- boston_scaled[-ind,]
lda.fit <- lda(crime ~., data = train)
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch= classes)
lda.arrows(lda.fit, myscale = 1)
# we save the correct classes from test data:
correct_classes <- test$crime
# remove the crime variable from the test data:
test <- dplyr::select(test, -crime)
# we save the correct classes from test data:
correct_classes <- test$crime
# remove the crime variable from the test data:
test <- dplyr::select(test, -crime)
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch= classes)
lda.arrows(lda.fit, myscale = 1)
# we save the correct classes from test data:
correct_classes <- test$crime
# remove the crime variable from the test data:
test <- dplyr::select(test, -crime)
# we save the correct classes from test data:
correct_classes <- test$crime
# remove the crime variable from the test data:
test <- dplyr::select(test, -crime)
# we save the correct classes from test data:
correct_classes <- test$crime
# remove the crime variable from the test data:
test <- dplyr::select(test, -crime)
# we save the correct classes from test data:
correct_classes <- test$crime
# remove the crime variable from the test data:
test <- dplyr::select(test, -crime)
boston_scaled <- dplyr::select(boston_scaled, -crim)
install.packages("MASS")
library(MASS)
str(Boston)
dim(Boston)
install.packages("corrplot")
install.packages("tidyverse")
library(corrplot)
library(tidyverse)
cor_matrix <- cor(Boston) %>% round(digits=2)
corrplot.mixed(cor_matrix, lower.col = "black", number.cex = .6)
summary(Boston)
gather(Boston) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()
boston_scaled <- scale(Boston)
summary(boston_scaled)
boston_scaled <- as.data.frame(boston_scaled)
# First we create the quantile vector of crim:
bins <- quantile(boston_scaled$crim)
bins
# Then we create the categorical variable "crime":
crime <- cut(boston_scaled$crim, breaks = bins, inlcude.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))
table(crime)
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
n <- nrow(boston_scaled)
# Choose randomly 80% of the rows:
ind <- sample(n, size = n*0.8)
# create the train set;
train <- boston_scaled[ind,]
# create the test set:
test <- boston_scaled[-ind,]
lda.fit <- lda(crime ~., data = train)
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch= classes)
lda.arrows(lda.fit, myscale = 1)
# we save the correct classes from test data:
correct_classes <- test$crime
# remove the crime variable from the test data:
test <- dplyr::select(test, -crime)
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
table(correct = correct_classes, predicted = lda.pred$class) %>% prop.table() %>% addmargins()
boston_scaled2 <- scale(Boston)
summary(boston_scaled2)
dist_eu <- dist(boston_scaled2)
km <- kmeans(boston_scaled2, centers = 3)
# avoid the kmeans to give us every time different results
set.seed(123)
# set the maximum number of clusters to 10
k_max <- 10
# calculate the total sum of squares:
twcss <- sapply(1:k_max, function(k){kmeans (boston_scaled2, k)$tot.wthinss})
# see graphically the optimal number of clusters:
qplot(x = 1:k_max, y= twcss, geom = 'line')
# change the object to data frame
boston_scaled2 <- as.data.frame(boston_scaled)
dist_eu <- dist(boston_scaled2)
dist_eu <- dist(boston_scaled2)
summary(dist_eu)
km <- kmeans(boston_scaled2, centers = 3)
# change the object to data frame
boston_scaled2 <- as.data.frame(boston_scaled)
dist_eu <- dist(boston_scaled2)
summary(dist_eu)
km <- kmeans(boston_scaled2, centers = 3)
km <- kmeans(boston_scaled2, centers = 3)
install.packages("MASS")
library(MASS)
str(Boston)
dim(Boston)
install.packages("corrplot")
install.packages("tidyverse")
library(corrplot)
library(tidyverse)
cor_matrix <- cor(Boston) %>% round(digits=2)
corrplot.mixed(cor_matrix, lower.col = "black", number.cex = .6)
summary(Boston)
gather(Boston) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()
boston_scaled <- scale(Boston)
summary(boston_scaled)
boston_scaled <- as.data.frame(boston_scaled)
# First we create the quantile vector of crim:
bins <- quantile(boston_scaled$crim)
bins
# Then we create the categorical variable "crime":
crime <- cut(boston_scaled$crim, breaks = bins, inlcude.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))
table(crime)
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
n <- nrow(boston_scaled)
# Choose randomly 80% of the rows:
ind <- sample(n, size = n*0.8)
# create the train set;
train <- boston_scaled[ind,]
# create the test set:
test <- boston_scaled[-ind,]
lda.fit <- lda(crime ~., data = train)
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch= classes)
lda.arrows(lda.fit, myscale = 1)
# we save the correct classes from test data:
correct_classes <- test$crime
# remove the crime variable from the test data:
test <- dplyr::select(test, -crime)
lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)
table(correct = correct_classes, predicted = lda.pred$class) %>% prop.table() %>% addmargins()
boston_scaled2 <- scale(Boston)
summary(boston_scaled2)
# change the object to data frame
boston_scaled2 <- as.data.frame(boston_scaled)
# change the object to data frame
boston_scaled2 <- as.data.frame(boston_scaled2)
dist_eu <- dist(boston_scaled2)
summary(dist_eu)
km <- kmeans(boston_scaled2, centers = 3)
summary(boston_scaled2)
colnames(boston_scaled2)
km <- kmeans(boston_scaled2, centers = 3, NA= FALSE)
?kmeans
boston_scaled2
km <- kmeans(boston_scaled2, centers = 3)
na.omit(boston_scaled2)
km <- kmeans(boston_scaled2, centers = 3)
na.omit(boston_scaled2)
km <- kmeans(boston_scaled2, centers = 3)
na.omit(boston_scaled2)
is.na(boston_scaled2)
na.omit(boston_scaled2)
row.has.na <- apply(boston_scaled2, 1, function(x){any(is.na(x))})
row.has.na <- apply(boston_scaled2, 1, function(x){any(is.na(x))})
sum(row.has.na)
row.has.na <- apply(boston_scaled2, 1, function(x){any(is.na(x))})
sum(row.has.na)
boston_scaled2 <- boston_scaled2[!row.has.na,]
km <- kmeans(boston_scaled2, centers = 3)
row.has.na <- apply(boston_scaled2, 1, function(x){any(is.na(x))})
sum(row.has.na)
boston_scaled2 <- boston_scaled2[!row.has.na,]
row.has.nan <- apply(boston_scaled2, 1, function(x){any(is.nan(x))})
sum(row.has.nan)
boston_scaled2 <- boston_scaled2[!row.has.nan,]
km <- kmeans(boston_scaled2, centers = 3)
row.has.na <- apply(boston_scaled2, 1, function(x){any(is.na(x))})
sum(row.has.na)
boston_scaled2 <- boston_scaled2[!row.has.na,]
row.has.nan <- apply(boston_scaled2, 1, function(x){any(is.nan(x))})
sum(row.has.nan)
boston_scaled2 <- boston_scaled2[!row.has.nan,]
row.has.inf <- apply(boston_scaled2, 1, function(x){any(is.infinite(x))})
sum(row.has.infinite)
row.has.na <- apply(boston_scaled2, 1, function(x){any(is.na(x))})
sum(row.has.na)
boston_scaled2 <- boston_scaled2[!row.has.na,]
row.has.nan <- apply(boston_scaled2, 1, function(x){any(is.nan(x))})
sum(row.has.nan)
boston_scaled2 <- boston_scaled2[!row.has.nan,]
row.has.inf <- apply(boston_scaled2, 1, function(x){any(is.infinite(x))})
sum(row.has.inf)
boston_scaled2 <- boston_scaled2[!row.has.inf,]
km <- kmeans(boston_scaled2, centers = 3)
# avoid the kmeans to give us every time different results
set.seed(123)
# set the maximum number of clusters to 10
k_max <- 10
# calculate the total sum of squares:
twcss <- sapply(1:k_max, function(k){kmeans (boston_scaled2, k)$tot.wthinss})
summary(Boston)
summary(Boston)
boston_scaled3 <- scale(Boston)
summary(boston_scaled3)
km2 <- kmeans(boston_scaled3, center = 3)
lda.fit2 <- lda(km2$cluster ~., data=boston_scaled3)
km2 <- kmeans(boston_scaled3, center = 3)
boston_scaled3 <- as.data.frame(boston_scaled3)
lda.fit2 <- lda(km2$cluster ~., data=boston_scaled3)
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(km2$cluster)
# plot the lda results
plot(lda.fit2, dimen = 2, col=classes, pch= classes)
lda.arrows(lda.fit, myscale = 1)
model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)
install. packages("plotly")
library(plotly)
install.packages("plotly")
library(plotly)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', col= train$crime)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', col= train$crime)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', col= km$cluster)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', col= train$crime)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', col= km2$cluster)
km <- kmeans(boston_scaled2, centers = 2)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', col= train$crime)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', col= km$cluster)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = = train$crime)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = = train$crime)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = train$crime)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = km$cluster)
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv", stringsAsFactors = F)
gii <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv", stringsAsFactors = F, na.strings = "..")
str(hd)
dim(hd)
str(gii)
dim(gii)
summary(hd)
summary(gii)
summary(hd)
summary(gii)
str(hd)
dim(hd)
summary(hd)
str(gii)
dim(gii)
summary(gii)
colnames(hd)[1] <- "HDI.rank"
colnames(hd)[2] <- "country"
colnames(hd)[3] <- "HDI"
colnames(hd)[4] <- "lifexp"
colnames(hd)[5] <- "yr.eduexp"
colnames(hd)[6] <- "meanyr.edu"
colnames(hd)[7] <- "GNI.cap"
colnames(hd)[8] <- "GNIrank-HDIrank"
colnames(hd)[1] <- "GII.rank"
colnames(hd)[2] <- "country"
colnames(hd)[3] <- "GII"
colnames(hd)[4] <- "matermor"
colnames(hd)[5] <- "adobirth"
colnames(hd)[6] <- "repreparl"
colnames(hd)[7] <- "pop.secondF"
colnames(hd)[8] <- "pop.secondM"
colnames(hd)[9] <- "labforceF"
colnames(hd)[10] <- "labforceM"
colnames(hd)
colnames(hd)[1] <- "HDI.rank"
colnames(hd)[2] <- "country"
colnames(hd)[3] <- "HDI"
colnames(hd)[4] <- "lifexp"
colnames(hd)[5] <- "yr.eduexp"
colnames(hd)[6] <- "meanyr.edu"
colnames(hd)[7] <- "GNI.cap"
colnames(hd)[8] <- "GNIrank-HDIrank"
colnames(gii)[1] <- "GII.rank"
colnames(gii)[2] <- "country"
colnames(gii)[3] <- "GII"
colnames(gii)[4] <- "matermor"
colnames(gii)[5] <- "adobirth"
colnames(gii)[6] <- "repreparl"
colnames(gii)[7] <- "pop.secondF"
colnames(gii)[8] <- "pop.secondM"
colnames(gii)[9] <- "labforceF"
colnames(gii)[10] <- "labforceM"
colnames(hd)
colnames(gii)
colnames(gii)[1] <- "GII.rank"
colnames(gii)[2] <- "country"
colnames(gii)[3] <- "GII"
colnames(gii)[4] <- "matermor"
colnames(gii)[5] <- "adobirth"
colnames(gii)[6] <- "repreparl"
colnames(gii)[7] <- "edu2F"
colnames(gii)[8] <- "edu2M"
colnames(gii)[9] <- "labforceF"
colnames(gii)[10] <- "labforceM"
gii <- mutate(gii, ratio.edu2 = gii$edu2F/gii$edu2M)
gii <- mutate(gii, ratio.labforce = gii$labforceF/gii$labforceM)
colnames(gii)
summary(gii$ratio.edu2)
summary(gii$ratio.labforce)
library("dplyr", lib.loc="~/R/win-library/3.4")
human <- inner_join(hd, gii, by = "country")
glimpse(human)
setwd("C:/Users/Elisabet/Desktop/ELI/MASTER'S DEGREE/Open Data Science/GitHub/IODS-project/IODS-project")
setwd("C:/Users/Elisabet/Desktop/ELI/MASTER'S DEGREE/Open Data Science/GitHub/IODS-project/IODS-project/Data")
write.table(human, file = "human.csv", sep = ",", col.names = TRUE)
